{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LF30qK9odHRn",
        "outputId": "aa511ec8-a388-4879-86eb-6e555ebfeb5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "2lS3fLwwA4m-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import csv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def get_validation_set_and_process_data(path):\n",
        "  validation_set = pd.read_csv(path)\n",
        "\n",
        "  num_examples = validation_set.shape[0]\n",
        "  label = np.zeros((1, num_examples))\n",
        "\n",
        "  # Shape of each 1 by 7 array\n",
        "  shape_of_each_example = (1, 24)\n",
        "  # Create the NumPy array filled with zeros\n",
        "  matrix = np.zeros((num_examples, shape_of_each_example[1]))\n",
        "\n",
        "  for index, row in validation_set.iterrows():\n",
        "    if ( str(row['label_boolean']) == \"True\"):\n",
        "      label[0,index] = 1\n",
        "    if ( str(row['label_boolean']) == \"False\"):\n",
        "      label[0,index] = 0\n",
        "\n",
        "    boolean_array = check_words_presence(row['text'])\n",
        "    numpy_array = np.array(boolean_array, dtype=int)\n",
        "    numpy_array = np.insert(numpy_array, 0, row['metaphorID'], axis=0)\n",
        "    parts_of_speech_freq = get_part_0f_speech_frequencies(row['text'])\n",
        "    target_encoding_and_pos_encoding = np.concatenate((numpy_array, parts_of_speech_freq))\n",
        "    marker_freq_array = get_marker_counts(row['text'])\n",
        "    final_feature = np.concatenate((target_encoding_and_pos_encoding, marker_freq_array))\n",
        "    matrix[index] = final_feature\n",
        "\n",
        "    # print(num_examples)\n",
        "    # return matrix, label\n",
        "\n",
        "\n",
        "  return matrix, label\n",
        "\n",
        "\n",
        "nlp_processor = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "qSL7kCUEdZ-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#encodes the metaphorical target with 1 and the rest of the words if not found with zero\n",
        "def check_words_presence(sentence):\n",
        "    words_to_check = [\"road\", \"candle\", \"light\", \"spice\", \"ride\", \"train\", \"boat\"]\n",
        "    sentence = sentence.lower()  # Convert to lowercase for case-insensitive matching\n",
        "    return [word in sentence for word in words_to_check]\n",
        "\n",
        "def get_part_0f_speech_frequencies(doc):\n",
        "    doc = nlp_processor(doc)\n",
        "    pos_frequencies = np.zeros(8)\n",
        "\n",
        "    # Define a mapping of part of speech labels to indices\n",
        "    pos_mapping = {\"NOUN\": 0, \"VERB\": 1, \"ADJ\": 2, \"ADV\": 3, \"PRON\": 4, \"ADP\": 5, \"CONJ\": 6, \"NUM\": 7}\n",
        "\n",
        "    # Count the frequencies of each part of speech\n",
        "    for token in doc:\n",
        "      pos = token.pos_\n",
        "      if pos in pos_mapping:\n",
        "          pos_index = pos_mapping[pos]\n",
        "          pos_frequencies[pos_index] += 1\n",
        "\n",
        "    return pos_frequencies\n",
        "    # Print the one-dimensional array of part of speech frequencies\n",
        "    print(\"Part of Speech Frequencies:\", pos_frequencies)\n",
        "\n",
        "def get_marker_counts(sentence):\n",
        "  metaphor_marker = [\"!\", \"as\", \"believe\", \"just\", \"like\", \"could\", \"really\", \"would\"]\n",
        "\n",
        "\n",
        "\n",
        "  # Process the sentence using spaCy\n",
        "  doc = nlp_processor(sentence)\n",
        "\n",
        "  # Initialize a list to store the counts\n",
        "  counts = [0] * len(metaphor_marker)\n",
        "\n",
        "  # Loop through the tokens in the processed sentence\n",
        "  for token in doc:\n",
        "      # Check if the token text is in the list of words to count\n",
        "      if token.text in metaphor_marker:\n",
        "          # Find the index of the token text in the list and increment the corresponding count\n",
        "          index = metaphor_marker.index(token.text)\n",
        "          counts[index] += 1\n",
        "\n",
        "  # Convert the list of counts to a NumPy array\n",
        "  counts_array = np.array(counts)\n",
        "\n",
        "  return counts_array"
      ],
      "metadata": {
        "id": "fOgV1RCQgUiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def get_predictions(X, Y):\n",
        "  w1 = np.load('/content/drive/MyDrive/w1_c.npy', allow_pickle=True)\n",
        "  b1 = np.load('/content/drive/MyDrive/b1_c.npy', allow_pickle=True)\n",
        "  w2 = np.load('/content/drive/MyDrive/w2_c.npy', allow_pickle=True)\n",
        "  b2 = np.load('/content/drive/MyDrive/b2_c.npy', allow_pickle=True)\n",
        "\n",
        "  z1 = np.dot(w1, X.T)\n",
        "  a1 = sigmoid(z1)\n",
        "  z2 = np.dot(w2,a1)\n",
        "  a2 = sigmoid(z2)\n",
        "\n",
        "  # Threshold value\n",
        "  threshold = 0.4\n",
        "  # Convert values below the threshold to 0 and values above the threshold to 1\n",
        "  predicted_label = (a2 > threshold).astype(int)\n",
        "\n",
        "  return predicted_label"
      ],
      "metadata": {
        "id": "1KveRwigmjfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_accuracy(y_true, y_pred):\n",
        "    # Ensure the arrays have the same shape\n",
        "    if y_true.shape != y_pred.shape:\n",
        "        raise ValueError(\"Input arrays must have the same shape.\")\n",
        "\n",
        "    # # Compare the two arrays element-wise and count the number of matches\n",
        "    # correct_predictions = np.sum(np.isclose(y_true, y_pred, rtol=1e-05, atol=1e-08))\n",
        "\n",
        "    # Calculate the accuracy as the percentage of correct predictions\n",
        "    accuracy = (y_true == y_pred).mean() * 100.0\n",
        "    return accuracy\n"
      ],
      "metadata": {
        "id": "-6rOH2m5o7mU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_confusion_matrix(true_labels, predicted_labels):\n",
        "    # Ensure the inputs are NumPy arrays\n",
        "    true_labels = np.array(true_labels)\n",
        "    predicted_labels = np.array(predicted_labels)\n",
        "\n",
        "    # True Positive (TP)\n",
        "    tp = np.sum((true_labels == 1) & (predicted_labels == 1))\n",
        "\n",
        "    # False Positive (FP)\n",
        "    fp = np.sum((true_labels == 0) & (predicted_labels == 1))\n",
        "\n",
        "    # True Negative (TN)\n",
        "    tn = np.sum((true_labels == 0) & (predicted_labels == 0))\n",
        "\n",
        "    # False Negative (FN)\n",
        "    fn = np.sum((true_labels == 1) & (predicted_labels == 0))\n",
        "\n",
        "    # Precision\n",
        "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
        "\n",
        "    # Recall (Sensitivity)\n",
        "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
        "\n",
        "    # F1 Score\n",
        "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
        "\n",
        "\n",
        "    return precision, recall, f1_score"
      ],
      "metadata": {
        "id": "U5qJm1aFpkq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/drive/MyDrive/stat_ml_data/validation_set.csv'\n",
        "# path = '/content/drive/MyDrive/stat_ml_data/training_set.csv'\n",
        "processed_data, label = get_validation_set_and_process_data(path)\n",
        "\n",
        "y_pred = get_predictions(processed_data,label)\n",
        "# print(processed_data.shape, label.shape)\n",
        "# processed_data, label = get_features_from_validation_set(validation_set)\n",
        "# print(validation_set.head)\n",
        "# print(label)\n",
        "# print(processed_data)"
      ],
      "metadata": {
        "id": "iGv3siOTgtv6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(label.shape)\n",
        "# print(y_pred.shape)\n",
        "# print(label)\n",
        "# print(y_pred)\n",
        "accuracy = calculate_accuracy(label, y_pred)\n",
        "precision, recall, f1_score = calculate_confusion_matrix(label, y_pred)\n",
        "print(f\"Precision: {precision:.2f}%\")\n",
        "print(f\"recall: {recall:.2f}%\")\n",
        "print(f\"Accuracy: {accuracy:.2f}%\")\n",
        "print(f\"f1 score: {f1_score:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "398azX64o-ZP",
        "outputId": "fcb3c481-717e-4878-a336-182af6c71c7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.75%\n",
            "recall: 0.95%\n",
            "Accuracy: 73.80%\n",
            "f1 score: 0.84%\n"
          ]
        }
      ]
    }
  ]
}